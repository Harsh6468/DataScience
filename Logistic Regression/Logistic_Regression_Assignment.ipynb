{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9dd6d8d",
   "metadata": {},
   "source": [
    "## **Logistic Regression Assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9355f",
   "metadata": {},
   "source": [
    "## **Theory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba326bd",
   "metadata": {},
   "source": [
    "### **1. What is Logistic Regression, and how does it differ from Linear Regression?**\n",
    "Logistic Regression is a classification algorithm used when the dependent variable is categorical where as Linear Regression can predicts continuous values. Logistic Regression predicts probabilities using the sigmoid function to map predictions to binary outputs (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ee098",
   "metadata": {},
   "source": [
    "### **2. What is the mathematical equation of Logistic Regression?**\n",
    "In Logistic Regression, we model the **probability** that a given input $x$ belongs to class 1 (i.e., $P(y = 1 | x)$) using the **sigmoid (logistic) function** applied to a linear combination of input features:\n",
    "\n",
    "$$ h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $h_\\theta(x)$ is the predicted probability that $y = 1$ given input $x$\n",
    "* $\\theta$ is the vector of model parameters (weights)\n",
    "* $x$ is the feature vector\n",
    "* $\\theta^T x$ is the dot product of $\\theta$ and $x$\n",
    "\n",
    "The decision boundary is typically at $h_\\theta(x) = 0.5$, i.e.,\n",
    "\n",
    "* If $h_\\theta(x) \\geq 0.5$, predict class **1**\n",
    "* If $h_\\theta(x) < 0.5$, predict class **0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc198ae",
   "metadata": {},
   "source": [
    "### **3. Why do we use the Sigmoid function in Logistic Regression**\n",
    "The sigmoid function maps any real-valued number to a value between 0 and 1, which can be interpreted as a probability. This is essential for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042bc19f",
   "metadata": {},
   "source": [
    "### **4. What is the cost function of Logistic Regression**\n",
    "The **cost function** in Logistic Regression is used to evaluate how well the model's predicted probabilities match the actual class labels. It is based on **log loss** or **binary cross-entropy**, defined as:\n",
    "\n",
    "$$ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))] $$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $m$: Number of training examples\n",
    "* $y^{(i)}$: Actual label (0 or 1) for example $i$\n",
    "* $h_\\theta(x^{(i)})$: Predicted probability that $y = 1$ given input $x^{(i)}$\n",
    "* $\\log$: Natural logarithm\n",
    "\n",
    "### Intuition:\n",
    "* If the prediction is close to the true label, the cost is low.\n",
    "* If the prediction is far from the true label, the cost is high.\n",
    "* The function penalizes confident but wrong predictions heavily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae63f99",
   "metadata": {},
   "source": [
    "### **5. What is Regularization in Logistic Regression? Why is it needed?**\n",
    "Regularization is a technique to prevent overfitting by adding a penalty term to the cost function. It helps to keep the model weights small and generalizes better on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d93baa",
   "metadata": {},
   "source": [
    "### **6. Explain the difference between Lasso, Ridge, and Elastic Net regression.**\n",
    "- **Lasso (L1)**: Adds absolute value of coefficients (sparse solutions).\n",
    "- **Ridge (L2)**: Adds squared value of coefficients (shrinks coefficients).\n",
    "- **Elastic Net**: Combines L1 and L2 penalties.\n",
    "\n",
    "| Method      | Penalty Type | Feature Selection | Handles Multicollinearity | Coefficients Can Be Zero |\n",
    "| ----------- | ------------ | ----------------- | ------------------------- | ------------------------ |\n",
    "| Ridge       | L2           | No              | Yes                     | No                     |\n",
    "| Lasso       | L1           | Yes             | No                     | Yes                    |\n",
    "| Elastic Net | L1 + L2      | Yes             | Yes                     | Yes                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720222e",
   "metadata": {},
   "source": [
    "### **7. When should we use Elastic Net instead of Lasso or Ridge?**\n",
    "Use **Elastic Net Regression** when you want to **combine the strengths of both Lasso and Ridge**, especially in the following scenarios:\n",
    "\n",
    "#### **High-Dimensional Data (p > n)**\n",
    "* When the number of features $p$ exceeds the number of samples $n$, Lasso may select too few features.\n",
    "* Elastic Net provides a more stable selection by combining L1 and L2 penalties.\n",
    "\n",
    "#### **Correlated Features**\n",
    "* Lasso tends to select one feature from a group of correlated ones and ignore the rest.\n",
    "* Ridge keeps all of them but does not perform feature selection.\n",
    "* **Elastic Net** can select groups of correlated features together, which is often desired.\n",
    "\n",
    "#### **Sparse + Grouping Effect**\n",
    "* You want the **sparsity** of Lasso (i.e., feature selection) **and** the **grouping behavior** of Ridge (i.e., correlated features get similar coefficients).\n",
    "\n",
    "#### **When Lasso or Ridge Alone Underperform**\n",
    "* If Lasso underfits (too sparse) or Ridge overfits (no feature elimination), Elastic Net often performs better by tuning the balance between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe662c",
   "metadata": {},
   "source": [
    "### **8. What is the impact of the regularization parameter (Î») in Logistic Regression?**\n",
    "A higher Î» means stronger regularization (smaller weights), which can reduce overfitting but may underfit if too strong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe58d2",
   "metadata": {},
   "source": [
    "### **9. What are the key assumptions of Logistic Regression**\n",
    "- The dependent variable is binary.\n",
    "- No multicollinearity among independent variables.\n",
    "- Linearity between independent variables and log odds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7f1ed",
   "metadata": {},
   "source": [
    "### **10. What are some alternatives to Logistic Regression for classification tasks?**\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Support Vector Machines (SVM)\n",
    "- k-Nearest Neighbors (k-NN)\n",
    "- Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f063b55",
   "metadata": {},
   "source": [
    "### **11. What are Classification Evaluation Metrics?**\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "- ROC-AUC Score\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5aa16",
   "metadata": {},
   "source": [
    "### **12. How does class imbalance affect Logistic Regression?**\n",
    "It can lead to a biased model that favors the majority class. Class imbalance occurs when one class significantly outnumbers the other(s) in a classification dataset (e.g., 95% class 0 and 5% class 1).\n",
    "* Biased Predictions:\n",
    "    - The model may predict the majority class more often to maximize accuracy.\n",
    "    - This results in high accuracy but poor recall/precision for the minority class.\n",
    "\n",
    "* Poor Learning of Minority Class:\n",
    "    - The model may treat the minority class as noise.\n",
    "    - Fails to learn distinguishing patterns for the underrepresented class.\n",
    "\n",
    "* Misleading Evaluation Metrics:\n",
    "    - Accuracy becomes unreliable because predicting the majority class alone gives high accuracy.\n",
    "    - For example, predicting all \"0\" in a 95:5 class distribution gives 95% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c319f05",
   "metadata": {},
   "source": [
    "### **13. What is Hyperparameter Tuning in Logistic Regression?**\n",
    "Hyperparameter tuning is the process of finding the optimal settings (hyperparameters) that control the behavior of a Logistic Regression model â€” before training begins.\n",
    "\n",
    "| Hyperparameter | Description                                                                |\n",
    "| -------------- | -------------------------------------------------------------------------- |\n",
    "| `C`            | Inverse of regularization strength (smaller `C` = stronger regularization) |\n",
    "| `penalty`      | Type of regularization: `'l1'`, `'l2'`, `'elasticnet'`, or `'none'`        |\n",
    "| `solver`       | Optimization algorithm: `'liblinear'`, `'saga'`, `'lbfgs'`, `'newton-cg'`  |\n",
    "| `max_iter`     | Maximum number of iterations for convergence                               |\n",
    "| `class_weight` | Handling class imbalance (`None` or `'balanced'`)                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ed0ab",
   "metadata": {},
   "source": [
    "### **14. What are different solvers in Logistic Regression? Which one should be used?**\n",
    "- **liblinear**: good for small datasets, supports L1.\n",
    "- **saga**: supports all penalties and works with large datasets.\n",
    "- **lbfgs**: fast for L2 penalty and multiclass classification.\n",
    "\n",
    "| Solver        | Supports L1? | Supports L2? | Supports Elastic Net? | Handles Multiclass? | Suitable For         |\n",
    "| ------------- | ------------ | ------------ | --------------------- | ------------------- | -------------------- |\n",
    "| `'liblinear'` | Yes        | Yes        | No                  | OvR only          | Small datasets       |\n",
    "| `'saga'`      | Yes        | Yes        | Yes                 | OvR & multinomial | Large + sparse data  |\n",
    "| `'lbfgs'`     | No         | Yes        | No                  | OvR & multinomial | Medium to large data |\n",
    "| `'newton-cg'` | No         | Yes        | No                  | OvR & multinomial | Large datasets       |\n",
    "| `'sag'`       | No         | Yes        | No                  | OvR & multinomial | Large + sparse data  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af7b5d",
   "metadata": {},
   "source": [
    "### **15. How is Logistic Regression extended for multiclass classification?**\n",
    "By default, Logistic Regression is designed for binary classification (two classes). However, it can be extended to handle multiclass classification problems using the following strategies:\n",
    "* One-vs-Rest (OvR) (a.k.a. One-vs-All)\n",
    "    - Trains one binary classifier per class.\n",
    "    - For class ð‘˜, the classifier is trained to distinguish class ð‘˜ vs. all other classes.\n",
    "    - During prediction, the class with the highest confidence score is chosen.\n",
    "* Multinomial (Softmax) Logistic Regression\n",
    "    - Trains a single model to predict the probability distribution over all classes.\n",
    "    - Uses the softmax function (generalization of sigmoid) in the final layer.\n",
    "\n",
    "$$\n",
    "P(y = k \\mid x) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^K e^{\\theta_j^T x}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115694c",
   "metadata": {},
   "source": [
    "### **16. What are the advantages and disadvantages of Logistic Regression?**\n",
    "**Advantages**:\n",
    "- Simple and interpretable\n",
    "- Efficient for binary classification\n",
    "\n",
    "**Disadvantages**:\n",
    "- Assumes linear relationship\n",
    "- Poor performance with non-linear data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199e87c",
   "metadata": {},
   "source": [
    "### **17. What are some use cases of Logistic Regression?**\n",
    "- Spam detection\n",
    "- Disease prediction\n",
    "- Credit scoring\n",
    "- Marketing (customer churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c103ce6",
   "metadata": {},
   "source": [
    "### **18. What is the difference between Softmax Regression and Logistic Regression?**\n",
    "Here's a clear explanation of the difference between **Softmax Regression** and **Logistic Regression**:\n",
    "\n",
    "#### Logistic Regression\n",
    "* **Type:** Binary classifier.\n",
    "* **Purpose:** Used when the target variable has **two classes** (e.g., yes/no, 0/1).\n",
    "* **Output:** Produces a probability for the **positive class** using the logistic (sigmoid) function:\n",
    "  $$\n",
    "  P(y=1 \\mid \\mathbf{x}) = \\frac{1}{1 + e^{-(\\mathbf{w}^\\top \\mathbf{x} + b)}}\n",
    "  $$\n",
    "* **Decision:** Usually, if $P(y=1) > 0.5$, predict class 1; otherwise class 0.\n",
    "\n",
    "#### Softmax Regression (Multinomial Logistic Regression)\n",
    "* **Type:** Multiclass classifier.\n",
    "* **Purpose:** Used when the target variable has **more than two classes** (e.g., 3 or more categories).\n",
    "* **Output:** Produces a probability distribution over all classes using the **softmax function**:\n",
    "  For $K$ classes, for class $j$:\n",
    "  $$\n",
    "  P(y=j \\mid \\mathbf{x}) = \\frac{e^{\\mathbf{w}_j^\\top \\mathbf{x} + b_j}}{\\sum_{k=1}^K e^{\\mathbf{w}_k^\\top \\mathbf{x} + b_k}}\n",
    "  $$\n",
    "\n",
    "* **Decision:** Predict the class with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3f4e2",
   "metadata": {},
   "source": [
    "### **19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**\n",
    "Choosing between **One-vs-Rest (OvR)** and **Softmax Regression** for multiclass classification depends on several factors like model complexity, performance, and dataset characteristics. Hereâ€™s a clear comparison and guidance on how to choose:\n",
    "\n",
    "#### One-vs-Rest (OvR) Approach\n",
    "\n",
    "* What it is:\n",
    "    Decomposes a multiclass problem into multiple binary classification problems. For $K$ classes, train $K$ binary classifiers, each distinguishing one class from the rest.\n",
    "\n",
    "* How it works:\n",
    "    Each classifier predicts the probability that the input belongs to its class vs. all others. At prediction time, choose the class with the highest classifier score.\n",
    "\n",
    "* Advantages:\n",
    "  * Simpler to implement, especially if you already have a binary classifier.\n",
    "  * Can use different classifiers for each class.\n",
    "  * Sometimes works better with imbalanced classes.\n",
    "  * Training can be parallelized easily since classifiers are independent.\n",
    "\n",
    "* Disadvantages:\n",
    "  * Classifiers are trained independently, so probabilities might not be well calibrated.\n",
    "  * Might produce inconsistent predictions because classifiers are separate.\n",
    "  * Potentially slower at prediction time (needs to run $K$ classifiers).\n",
    "\n",
    "#### Softmax (Multinomial Logistic Regression)\n",
    "\n",
    "* What it is:\n",
    "  A single model that directly predicts the probability distribution over all classes using the softmax function.\n",
    "\n",
    "* How it works:\n",
    "  Trains one model with parameters for all classes simultaneously, optimizing the multinomial likelihood.\n",
    "\n",
    "* Advantages:\n",
    "  * Probabilities are normalized and consistent across classes.\n",
    "  * Often better performance since it models classes jointly.\n",
    "  * Generally better when classes are mutually exclusive.\n",
    "  * More mathematically elegant and efficient at prediction time (just one forward pass).\n",
    "* Disadvantages:\n",
    "  * Slightly more complex to implement and optimize.\n",
    "  * Can be less flexible if classes are not mutually exclusive.\n",
    "  * Training can be slower and more memory-intensive for very large $K$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b8ff14",
   "metadata": {},
   "source": [
    "### **20. How do we interpret coefficients in Logistic Regression?**\n",
    "In **Logistic Regression**, the coefficients represent the **impact of each feature** on the **log-odds** of the outcome.\n",
    "Logistic Regression models the **log-odds** (also called the logit) of the probability that the dependent variable equals 1:\n",
    "$$\n",
    "\\log\\left(\\frac{P(y=1)}{1 - P(y=1)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "* $\\beta_0$: Intercept\n",
    "* $\\beta_i$: Coefficient for feature $x_i$\n",
    "* **Sign**:\n",
    "  * If $\\beta_i > 0$: Increasing $x_i$ increases the **log-odds** of the positive class (i.e., increases the probability of $y = 1$).\n",
    "  * If $\\beta_i < 0$: Increasing $x_i$ **decreases** the probability of $y = 1$.\n",
    "* **Magnitude**:\n",
    "  * Larger absolute values mean a **stronger effect** on the outcome.\n",
    "* **Exponentiated Coefficient** $e^{\\beta_i}$:\n",
    "  * This gives the **odds ratio** for a **1-unit increase** in $x_i$, holding all other variables constant.\n",
    "  * If $e^{\\beta_i} = 1.5$, then a 1-unit increase in $x_i$ multiplies the odds of $y = 1$ by 1.5.\n",
    "  * If $e^{\\beta_i} = 0.6$, then a 1-unit increase in $x_i$ multiplies the odds by 0.6 (i.e., lowers the odds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74728fec",
   "metadata": {},
   "source": [
    "## **Practical**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac76d6",
   "metadata": {},
   "source": [
    "### **1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114aad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc54075",
   "metadata": {},
   "source": [
    "### **2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ecbf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Regularized Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=10000)\n",
    "model_l1.fit(X_train, y_train)\n",
    "y_pred_l1 = model_l1.predict(X_test)\n",
    "print(\"L1 Regularized Accuracy:\", accuracy_score(y_test, y_pred_l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba703a",
   "metadata": {},
   "source": [
    "### **3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3d2eac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Regularized Accuracy: 0.956140350877193\n",
      "Coefficients: [[ 2.04354028e+00  1.50727651e-01 -1.21865250e-01 -1.40712354e-03\n",
      "  -1.32867463e-01 -4.01738361e-01 -6.18552774e-01 -3.20877196e-01\n",
      "  -1.95151233e-01 -2.94653873e-02 -4.38412463e-02  1.43445492e+00\n",
      "  -2.90303534e-01 -7.31190089e-02 -1.44981250e-02 -9.69515060e-03\n",
      "  -4.97155954e-02 -3.54239504e-02 -3.98113534e-02  4.23459422e-03\n",
      "   1.20942508e+00 -3.99652497e-01 -4.43667770e-02 -2.63631821e-02\n",
      "  -2.44521885e-01 -1.18358468e+00 -1.55571752e+00 -5.78440045e-01\n",
      "  -6.97654198e-01 -1.15249374e-01]]\n"
     ]
    }
   ],
   "source": [
    "model_l2 = LogisticRegression(penalty='l2', solver='liblinear', max_iter=10000)\n",
    "model_l2.fit(X_train, y_train)\n",
    "y_pred_l2 = model_l2.predict(X_test)\n",
    "print(\"L2 Regularized Accuracy:\", accuracy_score(y_test, y_pred_l2))\n",
    "print(\"Coefficients:\", model_l2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9027b1f8",
   "metadata": {},
   "source": [
    "### **4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b6564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "model_en = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=10000)\n",
    "model_en.fit(X_train, y_train)\n",
    "y_pred_en = model_en.predict(X_test)\n",
    "print(\"Elastic Net Accuracy:\", accuracy_score(y_test, y_pred_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03567f",
   "metadata": {},
   "source": [
    "### **5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac9408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvR Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "model_ovr = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=10000)\n",
    "model_ovr.fit(X_train, y_train)\n",
    "print(\"OvR Accuracy:\", accuracy_score(y_test, model_ovr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6edd9ea",
   "metadata": {},
   "source": [
    "### **6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adcbd6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Accuracy after tuning: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Accuracy after tuning:\", accuracy_score(y_test, grid.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2b4fb",
   "metadata": {},
   "source": [
    "### **7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af8fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified K-Fold Average Accuracy: 0.9507995652848935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "scores = cross_val_score(LogisticRegression(max_iter=10000), X, y, cv=skf)\n",
    "print(\"Stratified K-Fold Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63110a03",
   "metadata": {},
   "source": [
    "### **8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17969a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8555798687089715\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"framingham.csv\")\n",
    "df = df.dropna()\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X, y)\n",
    "print(\"Accuracy:\", accuracy_score(y, model.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3030a",
   "metadata": {},
   "source": [
    "### **9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9656c4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 1}\n",
      "Best Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(LogisticRegression(max_iter=10000), param_dist, n_iter=5, cv=3)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Accuracy:\", accuracy_score(y_test, random_search.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58a50a",
   "metadata": {},
   "source": [
    "### **10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f52f25cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvO Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "ovo_model = OneVsOneClassifier(LogisticRegression(max_iter=10000))\n",
    "ovo_model.fit(X_train, y_train)\n",
    "print(\"OvO Accuracy:\", accuracy_score(y_test, ovo_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ff0df",
   "metadata": {},
   "source": [
    "### **11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "578a05c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e29eea",
   "metadata": {},
   "source": [
    "### **12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "941732fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9459459459459459\n",
      "Recall: 0.9859154929577465\n",
      "F1 Score: 0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3be7576",
   "metadata": {},
   "source": [
    "### **13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "178acce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Class Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "model_balanced = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
    "model_balanced.fit(X_train, y_train)\n",
    "print(\"Balanced Class Accuracy:\", accuracy_score(y_test, model_balanced.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dcbc9b",
   "metadata": {},
   "source": [
    "### **14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd1b61cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic Accuracy: 0.7988826815642458\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "\n",
    "titanic = titanic[[\"survived\", \"pclass\", \"sex\", \"age\"]].dropna()\n",
    "titanic[\"sex\"] = titanic[\"sex\"].map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "X = titanic[[\"pclass\", \"sex\", \"age\"]]\n",
    "y = titanic[\"survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Titanic Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f73989",
   "metadata": {},
   "source": [
    "### **15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd849eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without Scaling: 0.956140350877193\n",
      "Accuracy with Scaling: 0.9736842105263158\n",
      "Improvement in Accuracy: 0.0175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_raw = LogisticRegression(max_iter=10000)\n",
    "model_raw.fit(X_train, y_train)\n",
    "y_pred_raw = model_raw.predict(X_test)\n",
    "raw_accuracy = accuracy_score(y_test, y_pred_raw)\n",
    "print(\"Accuracy without Scaling:\", raw_accuracy)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_scaled = LogisticRegression(max_iter=10000)\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "scaled_accuracy = accuracy_score(y_test, y_pred_scaled)\n",
    "print(\"Accuracy with Scaling:\", scaled_accuracy)\n",
    "\n",
    "improvement = scaled_accuracy - raw_accuracy\n",
    "print(f\"Improvement in Accuracy: {improvement:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02008930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.8567948717948718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ef8ff",
   "metadata": {},
   "source": [
    "### **17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f302a3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with C=0.5: 0.7988826815642458\n"
     ]
    }
   ],
   "source": [
    "model_custom_c = LogisticRegression(C=0.5, max_iter=10000)\n",
    "model_custom_c.fit(X_train, y_train)\n",
    "print(\"Accuracy with C=0.5:\", accuracy_score(y_test, model_custom_c.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8c52c",
   "metadata": {},
   "source": [
    "### **18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81960731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean radius: -1.1518502613873867\n",
      "mean texture: 2.463260405141402\n",
      "mean perimeter: -0.02899692912636335\n"
     ]
    }
   ],
   "source": [
    "importance = model.coef_[0]\n",
    "for feature, coef in zip(data.feature_names, importance):\n",
    "    print(f\"{feature}: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a81db",
   "metadata": {},
   "source": [
    "### **19. Write a Python program to train Logistic Regression and evaluate its performance using Cohenâ€™s Kappa Score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0ef8fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Score: 0.9053470607771504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "print(\"Cohen's Kappa Score:\", cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29524fc",
   "metadata": {},
   "source": [
    "### **20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3781829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_scores = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "disp.plot()\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dea14041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liblinear solver accuracy: 0.956140350877193\n",
      "lbfgs solver accuracy: 0.956140350877193\n",
      "saga solver accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "for solver in ['liblinear', 'lbfgs', 'saga']:\n",
    "    model = LogisticRegression(solver=solver, max_iter=10000)\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"{solver} solver accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccffb28",
   "metadata": {},
   "source": [
    "### **22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84a781e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mattews Correlation Coefficient: 0.9068106119605033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "print(\"Mattews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09012618",
   "metadata": {},
   "source": [
    "### **23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c1cd390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Accuracy: 0.7988826815642458\n",
      "Scaled Accuracy: 0.7988826815642458\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "raw_model = LogisticRegression(max_iter=10000)\n",
    "raw_model.fit(X_train, y_train)\n",
    "raw_acc = accuracy_score(y_test, raw_model.predict(X_test))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "scaled_model = LogisticRegression(max_iter=10000)\n",
    "scaled_model.fit(X_train_scaled, y_train)\n",
    "scaled_acc = accuracy_score(y_test, scaled_model.predict(X_test_scaled))\n",
    "\n",
    "print(\"Raw Accuracy:\", raw_acc)\n",
    "print(\"Scaled Accuracy:\", scaled_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b687d",
   "metadata": {},
   "source": [
    "### **24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d684693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal C: 1\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Optimal C:\", grid.best_params_['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a31b4ab",
   "metadata": {},
   "source": [
    "### **25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52a45c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model Accuracy: 0.7988826815642458\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "dump(model, \"logreg_model.joblib\")\n",
    "\n",
    "loaded_model = load(\"logreg_model.joblib\")\n",
    "print(\"Loaded Model Accuracy:\", accuracy_score(y_test, loaded_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a08af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
